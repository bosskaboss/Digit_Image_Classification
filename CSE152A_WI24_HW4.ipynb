{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 152A Winter 2024 â€“ Assignment 4\n",
    "\n",
    "Assignment Published On: **Sat, Mar 2, 2024**\n",
    "\n",
    "Due On: **Tue, Mar 12, 2024 11:59 PM (Pacific Time)**\n",
    "\n",
    "Instructions:\n",
    "- Attempt all questions.\n",
    "- Please comment all your code adequately.\n",
    "- **Please install following packages in order to run the code: PyTorch, Torchvision, matplotlib, scikit-learn**\n",
    "- Please write your code at the ``WRITE YOUR CODE HERE'' prompt in the .ipynb file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIwR8JInyANK"
   },
   "source": [
    "# 1. Backpropogation [10 Points]\n",
    "\n",
    "We will study the backpropagation behavior for a  [sigmoid neuron](https://en.wikipedia.org/wiki/Sigmoid_function), given by:\n",
    "\n",
    "$$\n",
    "f(z) = \\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "\n",
    "Consider a two-dimensional input given by $x = (x_1, x_2)^T$. A weight vector $w = (w_1, w_2)^T$ and a bias $b$ act on it. Thus, the output of a neuron is given by $f(x_1, x_2) = \\frac{1}{1+e^{-(w_1x_1+w_2x_2+b)}}$.\n",
    "\\\n",
    "\\\n",
    "(a.) Draw the computational graph for the neuron in terms of elementary operations (addition, subtraction, multiplication, division, exponentiation) as seen in class. **[2 points]**\n",
    "\n",
    "(b.) Consider inputs $x_1 = 0.3, x_2=0.4,$ weights $w_1 = 0.3, w_2 = 0.6$ and bias $b = 0.2$. In the same figure, show the values at each node of the graph during forward propagation. **[2 points]**\n",
    "\n",
    "(c.) Use backpropagation to determine the gradients $\\frac{\\partial f}{\\partial w_1}, \\frac{\\partial f}{\\partial w_2}, \\frac{\\partial f}{\\partial b}$. Also illustrate in the same figure the intermediate gradients at each node of the computation graph. **[4 points]**\n",
    "\n",
    "(d.) Explain the process of backpropagation you used to compute partial derivatives. **[2 points]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rI7u_38usuUV"
   },
   "outputs": [],
   "source": [
    "# You can insert an image here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HTUBW4dyANM"
   },
   "source": [
    "# 2. Training a small CNN for MNIST digit classification [15 Points]\n",
    "\n",
    "In this problem, you will train a small convolutional neural network for image classification, using PyTorch. We will use the MNIST dataset for digit classification (http://yann.lecun.com/exdb/mnist/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rq_g-n1OyANN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2QYlNmYGyANN",
    "outputId": "4bbb3b56-71e3-4a45-8d35-58c6da6e4178"
   },
   "outputs": [],
   "source": [
    "# Load in the datasets\n",
    "\n",
    "# Download the MNIST Datasets (you will use these variables later on)\n",
    "MNIST_train = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "MNIST_test = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "# Code adapted from PyTorch https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "train_labels = MNIST_train.targets\n",
    "label = (train_labels == 0).nonzero()\n",
    "for i in range(1, cols * rows + 1):\n",
    "    # Select image of each label\n",
    "    indices = (train_labels == i-1).nonzero()\n",
    "    sample_idx = indices[0,0]\n",
    "    img, label = MNIST_train[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(i-1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Image Shape: {img.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jZ2p_GPEQbSj",
    "outputId": "32715f6f-2a0e-49b6-efb1-f8af2e71c014"
   },
   "outputs": [],
   "source": [
    "# Check device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQBE1JbPyANP"
   },
   "source": [
    "**[ 3 points ] Define the network structure as follows**\n",
    "\n",
    "* Convolutional layer with 32 kernels, window size 5, padding size 2, stride 1\n",
    "* In place ReLU activation layer\n",
    "* Max pooling layer with window size 2, stride 2\n",
    "* Convolutional layer with 64 kernels, window size 5, padding size 2, stride 1\n",
    "* In place ReLU activation layer\n",
    "* Max pooling layer with window size 2, stride 2\n",
    "* Fully connected layer with 1024 output channels\n",
    "* In place ReLU activation layer\n",
    "* Dropout layer with drop rate 0.4\n",
    "* Fully connected layer with 10 output channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Te0LYAY4yANQ",
    "outputId": "d96b5672-1e61-4dd7-8f74-9f693b964fbf"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,drop):\n",
    "        super(Net, self).__init__()\n",
    "        self.drop = drop\n",
    "        # DEFINE THE NETWORK STRUCTURE\n",
    "\n",
    "        # Example: self.conv1 = nn.Conv2d(1, 3, 5,stride=1,padding=2,bias=True)\n",
    "        # You can look at the main PyTorch tutorial for reference\n",
    "        # https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n",
    "        \n",
    "        # --------------- YOUR CODE HERE ---------------\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # --------------- YOUR CODE HERE ---------------\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Print net\n",
    "net = Net(drop=True).to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_rltLgZyANQ"
   },
   "source": [
    "**[ 5 points ] Complete the train function below. Use the same parameters to perform training in each of the following setups:**\n",
    "\n",
    "* SGD for optimization, without dropout\n",
    "* SGD for optimization, with dropout\n",
    "* Adam for optimization, without dropout\n",
    "* Adam for optimization, with dropout.\n",
    "\n",
    "As evaluation for each case above, perform the following:\n",
    "* Plot the loss graph and the accuracy graph on training set on the same plot\n",
    "* Print the accuracy on test set after training\n",
    "\n",
    "Test accuracies are expected to be quite high (~98 %) for all networks.\n",
    "\n",
    "Training can take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SN_0N0CDyANR"
   },
   "outputs": [],
   "source": [
    "# CODE BELOW IS AN EXAMPLE STARTER\n",
    "# FEEL FREE TO EDIT ANYTHING\n",
    "\n",
    "# 'to_train' is a parameter that determines what part of the net to train.\n",
    "# It is not required for this question, but will be useful in the next one.\n",
    "# You should also change the parameters: epochs, batch, and learning rate as necessary.\n",
    "# You may need to tune these hyperparameters.\n",
    "def train(train_dataset, net, to_train, opt, epochs=10, batch=200, learning_rate=1e-3):\n",
    "    # Initialize loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losslist = []\n",
    "    acclist=[]\n",
    "\n",
    "    # Create dataloader\n",
    "    MNIST_train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=True)\n",
    "\n",
    "    # Select optimizer\n",
    "    if(opt=='adam'):\n",
    "        optimizer = optim.Adam(to_train,lr=learning_rate)\n",
    "    else:\n",
    "        optimizer = optim.SGD(to_train,lr=learning_rate,momentum = 0.99)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Set model to training mode\n",
    "    net.train()\n",
    "    for k in tqdm(range(epochs)):\n",
    "        for it, (X,y) in enumerate(MNIST_train_dataloader):\n",
    "            # Send to device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Train the model using the optimizer and the batch data.\n",
    "            # Append the loss and accuracy from each iteration to the losslist and acclist arrays\n",
    "            # --------------- YOUR CODE HERE ---------------\n",
    "\n",
    "    return losslist,acclist\n",
    "\n",
    "# Used to test or evaluate your network. Already written for you.\n",
    "def test(test_dataset, net):\n",
    "    batch = 200\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch)\n",
    "    size = len(test_dataloader.dataset)\n",
    "\n",
    "    # Set model to eval mode\n",
    "    net.eval()\n",
    "\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            # Send to device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Prediction\n",
    "            pred = net(X)\n",
    "\n",
    "            # Calculate number of correct predictions in the batch\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    # Compute total accuracy\n",
    "    acc = correct / size\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "9P0oA7YpyANR",
    "outputId": "8bc5fb15-8c9d-446e-df5b-86a43291a0e8"
   },
   "outputs": [],
   "source": [
    "# SGD with no dropout\n",
    "# Example code\n",
    "net = Net(drop=False).to(device)\n",
    "loss1, acc1 = train(MNIST_train, net, net.parameters(), 'sgd')\n",
    "ax=range(len(loss1))\n",
    "plt.plot(ax, loss1, ax, acc1)\n",
    "plt.legend(['loss', 'accuracy'])\n",
    "plt.show()\n",
    "print('Accuracy:{}'.format(test(MNIST_test, net)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZbGhjbWFY-zc"
   },
   "outputs": [],
   "source": [
    "# SGD with dropout\n",
    "# --------------- YOUR CODE HERE ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6u7TFzXZAue"
   },
   "outputs": [],
   "source": [
    "# Adam with no dropout\n",
    "# --------------- YOUR CODE HERE ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GOb05IZdZBKv"
   },
   "outputs": [],
   "source": [
    "# Adam with dropout\n",
    "# --------------- YOUR CODE HERE ---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOjzlyG0yANR"
   },
   "source": [
    "**[ 5 points ] Plot the following graphs and note your observations**\n",
    "\n",
    "* Training loss graphs of SGDâˆ’dropout and Adamâˆ’dropout on the same plot.\n",
    "* Training loss graphs for Adam-dropout for 3 different values of batch sizes of 10, 200 and 500, on the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3PWIT9YkyANS"
   },
   "outputs": [],
   "source": [
    "# --------------- YOUR CODE HERE ---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nd0hbsHeyANS"
   },
   "source": [
    "**[ 2 points ] The learning rate is a key hyperparameter during training. For this question, do the following.**\n",
    "\n",
    "1. [ 1 point ] Train three models for three different values of the learning rate hyperparameter. Plot the loss graphs for training with these values of the learning rate on the same plot. Make sure that you change the hyperparameter enough such that there is a clear difference in the graphs and comment on the differences. Use SGD optimizer and no dropout.\n",
    "\n",
    "2. [ 1 point ] Repeat the above task, but this time, use dropout with SGD optimizer. Note down your observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uy7fWmahyANS"
   },
   "outputs": [],
   "source": [
    "# --------------- YOUR CODE HERE ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- YOUR CODE HERE ---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oTW2XulyANU"
   },
   "source": [
    "# 3. Transfer learning [15 Points]\n",
    "\n",
    "You will now visualize the effects of transfer learning by performing experiments using the SVHN dataset (http://ufldl.stanford.edu/housenumbers/) . Note that this is just to understand how transfer learning works, in practice it is generally used with very large datasets and complex networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir SVHN\n",
    "%cd SVHN\n",
    "!wget -nc http://ufldl.stanford.edu/housenumbers/train_32x32.mat\n",
    "%cd ..\n",
    "\n",
    "# Convert .mat files to np arrays\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "\n",
    "def load_data(path):\n",
    "    data = sio.loadmat(path)\n",
    "    return np.array(data['X']), np.array(data['y'])\n",
    "\n",
    "data, labels = load_data('SVHN/train_32x32.mat')\n",
    "\n",
    "data = data.transpose((3, 2, 0, 1))\n",
    "labels  = labels.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14Dx-gBeyANU"
   },
   "source": [
    " **[ 2 points ] Plot 3 random images corresponding to each label from the training data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- YOUR CODE HERE ---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the dataset into two parts, one with labels 0-4 and other with labels 5-9, we have provided this code for you. This should print the sizes of data and labels in each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data and labels into two sets corresponding to labels 0-4 and 5-9.\n",
    "data1 = np.zeros((0, 3, 32, 32))\n",
    "labels1 = []\n",
    "data2 = np.zeros((0, 3, 32, 32))\n",
    "labels2 = []\n",
    "\n",
    "## SVHN has labels in the range 1-10 and not 0-9. \n",
    "# Split data and labels for labels 0 to 4\n",
    "for i in range(5):\n",
    "    x = data[labels == i+1][:5000]\n",
    "    data1 = np.vstack((data1, x))\n",
    "    labels1 += [i] * len(x)\n",
    "\n",
    "# Split data and labels for labels 5 to 9\n",
    "for i in range(5, 10):\n",
    "    x = data[labels == i+1][:5000]\n",
    "    data2 = np.vstack((data2, x))\n",
    "    labels2 += [i] * len(x)\n",
    "\n",
    "## Neural networks always accept labels in the range 0 to n-1.\n",
    "## change data from cardinal to ordinal.\n",
    "labels1 = np.array(labels1)\n",
    "labels2 = np.array(labels2) - 5\n",
    "\n",
    "data1.shape, data2.shape, labels1.shape, labels2.shape\n",
    "\n",
    "## should print ((25000, 3, 32, 32), (24607, 3, 32, 32), (25000,), (24607,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJaTTDaWyANW"
   },
   "source": [
    "**[ 3 points ] Create a simple convolutional network to classify the training data. The network structure should be as follows:**\n",
    "\n",
    "1. Layer 1 - Convolutional layer with kernel size 4, Stride 2, Output channels 5, Relu activation\n",
    "2. Layer 2 - Convolutional layer with kernel size 4, Stride 1, Output channels 10, Relu avtication\n",
    "3. Layer 3 - Convolutional layer with kernel size 4, Stride 1, Output channels 20, Relu activation\n",
    "4. Layer 4 - Convolutional layer with kernel size 4, Stride 1, Output channels 40, Relu activation\n",
    "5. Layer 5 - Fully connected layer with 5 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0ju7XGiyANW",
    "outputId": "209e0453-e483-404c-d386-c533e337a669"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_labels=5):\n",
    "        super().__init__()\n",
    "        # --------------- YOUR CODE HERE ---------------\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # --------------- YOUR CODE HERE ---------------\n",
    "\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FJAsD94yANW"
   },
   "source": [
    "**[ 5 points ] Complete the train function below and follow the instructions**\n",
    "\n",
    "* Initialize the network, train the complete network (net.parameters) on data1 (The first 5 classes)\n",
    "* Plot the loss and accuracy graphs over training on the same plot\n",
    "* Print the final training accuracy as well**\n",
    "\n",
    "Set the learning rate, number of iterations and batch size such that the loss is gradually and smoothly decreasing and converging. The accuracy at the end of training must be around or greater than 55 %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdxXMhowyANW"
   },
   "outputs": [],
   "source": [
    "# to_train can be net.paramaters OR net.fc.parameters OR net.conv1.parameters so that only certain parts of the net are trained\n",
    "def train(tdata,tlabel,net,to_train):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losslist = []\n",
    "    acclist = [] # Hint: use argmax to find the index with the largest value\n",
    "\n",
    "    # YOU MAY NEED TO CHANGE THESE PARAMETERS TO IMPROVE ACCURACY\n",
    "    epochs=10\n",
    "    batch=500\n",
    "    learning_rate=1e-3\n",
    "    optimizer = optim.SGD(to_train,lr=learning_rate)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for k in tqdm(range(epochs)):\n",
    "        ## Shuffle the data\n",
    "        indices = np.arange(len(tdata))\n",
    "        np.random.shuffle(indices)\n",
    "        tdata = tdata[indices]\n",
    "        tlabel = tlabel[indices]\n",
    "\n",
    "        for l in range(int(len(tdata)/batch)):\n",
    "\n",
    "            inputs = torch.FloatTensor(tdata[l*batch:(l+1)*batch]).to(device)\n",
    "            targets = torch.LongTensor(tlabel[l*batch:(l+1)*batch]).to(device)\n",
    "            # --------------- YOUR CODE HERE ---------------\n",
    "\n",
    "\n",
    "\n",
    "    return losslist,acclist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJ--b_FXyANW"
   },
   "outputs": [],
   "source": [
    "# --------------- YOUR CODE HERE ---------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSNfTOWWyANX"
   },
   "source": [
    "**[ 2 points ] Without reinitializing the network, train only the fully connected layer (net.fc.parameters) now on data2 (The next 5 classes)**\n",
    "\n",
    "Do not change any hyper parameters such as learning rate or batch size. Plot the loss and accuracy and print the final values like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLq1HeGLyANX"
   },
   "outputs": [],
   "source": [
    "# --------------- YOUR CODE HERE ---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AOR4Z3jyANX"
   },
   "source": [
    "**[ 3 points ] Now repeat the process in the opposite order**\n",
    "\n",
    "* Initialize the net again, train the whole network on data2, generate the same plots as before\n",
    "* Then without reinitializing the net, train only the fully connected layer on data1 and generate the plots\n",
    "\n",
    "Do not change any hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MdepBBZeyANX"
   },
   "outputs": [],
   "source": [
    "# --------------- YOUR CODE HERE ---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVBKoAZSyANX"
   },
   "source": [
    "**[ 5 points ]**\n",
    "\n",
    "* Plot the accuracy vs iterations for the classifers trained to classify data1, via normal learning as well as transfer learning, on the same plot\n",
    "* Plot another graph for the classifiers trained to classify data2\n",
    "\n",
    "Explain the results obtained, based on the training regimen. Comment on why transfer learning worked/didn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "On7RnPNxyANX"
   },
   "outputs": [],
   "source": [
    "# --------------- YOUR CODE HERE ---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdRNyAh9yANX"
   },
   "source": [
    "**Optional**: Create a network with more layers, pooling layers, and more filters and try to increase accuracy as much as possible. Play around with the hyperparameters to understand how they affect the training process. No need to turn in anything for this."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
